{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b16b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['ts', 'ssl_subject', 'ssl_issuer', 'http_uri', 'http_user_agent'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m             df[col] \u001b[38;5;241m=\u001b[39m df[col]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mnan)  \u001b[38;5;66;03m# string 'nan' -> actual NaN\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# ---------- 5. Split features / label (stratified) ----------\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdns_query\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mssl_subject\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mssl_issuer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp_uri\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp_user_agent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     70\u001b[0m X_train_raw, X_test_raw, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m     71\u001b[0m     X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my\n\u001b[0;32m     72\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\firek\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:5603\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5456\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5457\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5464\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5465\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5466\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5467\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5468\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5601\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5602\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5605\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5609\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5610\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5611\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\firek\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:4810\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4808\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4809\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4810\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4813\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\firek\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:4852\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4850\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4851\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4852\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4853\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4855\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4856\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\firek\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:7136\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7136\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7137\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['ts', 'ssl_subject', 'ssl_issuer', 'http_uri', 'http_user_agent'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ---------- 1. Read & basic cleaning ----------\n",
    "df = pd.read_csv(\"D:/vhproj\\Ton_IoT\\data\\Ton_IoT_Network.csv\")\n",
    "\n",
    "# Replace '-' placeholders with NaN (very important)\n",
    "df.replace('-', np.nan, inplace=True)\n",
    "\n",
    "# If there are trailing/leading spaces in string cells, strip them:\n",
    "for c in df.select_dtypes(include=['object']).columns:\n",
    "    df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "# ---------- 2. Make sure label is numeric ----------\n",
    "# Header: ... , label, type  -> in your sample label is numeric (0/1) and \"type\" is text\n",
    "# If label is string like 'normal'/'attack', map it. Here we assume numeric already:\n",
    "if df['label'].dtype == object:\n",
    "    # attempt to convert\n",
    "    try:\n",
    "        df['label'] = pd.to_numeric(df['label'], errors='coerce')\n",
    "    except Exception:\n",
    "        # fallback map common text labels:\n",
    "        df['label'] = df['label'].map({'normal':0, 'attack':1}).astype(float)\n",
    "\n",
    "# Drop rows where label is missing after conversion\n",
    "df = df[~df['label'].isna()].copy()\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# ---------- 3. Feature selection decisions ----------\n",
    "# Many IPs, URIs, certificates, user agents are high-cardinality and noisy.\n",
    "# You can either drop them or frequency-encode them. Here we:\n",
    "drop_cols = ['ts']  # timestamp usually not useful as raw numeric here (optional)\n",
    "# common noisy textual columns to drop (customize as you wish)\n",
    "possible_drop = ['src_ip', 'dst_ip', 'http_uri', 'http_user_agent', 'ssl_subject', 'ssl_issuer',\n",
    "                 'weird_name', 'weird_addl', 'weird_notice']\n",
    "for c in possible_drop:\n",
    "    if c in df.columns:\n",
    "        drop_cols.append(c)\n",
    "\n",
    "df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "# ---------- 4. Convert numeric-like columns to numeric (coerce errors -> NaN) ----------\n",
    "# Try to coerce any column that looks numeric when strings are present\n",
    "for col in df.columns:\n",
    "    if col == 'label':\n",
    "        continue\n",
    "    # if dtype is object but values look numeric, convert\n",
    "    if df[col].dtype == object:\n",
    "        # check fraction of values that can be parsed as numeric\n",
    "        parsed = pd.to_numeric(df[col], errors='coerce')\n",
    "        frac_numeric = parsed.notna().mean()\n",
    "        if frac_numeric >= 0.7:\n",
    "            # treat as numeric\n",
    "            df[col] = parsed\n",
    "        else:\n",
    "            # leave as object (categorical/text)\n",
    "            df[col] = df[col].replace('nan', np.nan)  # string 'nan' -> actual NaN\n",
    "\n",
    "# ---------- 5. Split features / label (stratified) ----------\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label']\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train_raw.shape, \"Test size:\", X_test_raw.shape)\n",
    "print(\"Label distribution (train):\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"Label distribution (test):\\n\", y_test.value_counts(normalize=True))\n",
    "\n",
    "# ---------- 6. Preprocessing functions ----------\n",
    "def preprocess_fit_transform(X_train):\n",
    "    \"\"\"\n",
    "    Fit imputers/encoders on X_train and transform X_train -> numeric matrix.\n",
    "    Strategy:\n",
    "      - Numeric cols: impute median, scale (for logistic; scaling optional for tree models)\n",
    "      - Categorical cols:\n",
    "         * If nunique <= 20 -> one-hot via pandas.get_dummies (keeps simple)\n",
    "         * Else (high cardinality) -> frequency encoding (category -> frequency in train)\n",
    "    Returns: X_train_proc (DataFrame), and dicts needed to transform test set\n",
    "    \"\"\"\n",
    "    X = X_train.copy()\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # numeric imputer + scaler\n",
    "    num_imputer = SimpleImputer(strategy='median')\n",
    "    X_num = pd.DataFrame(\n",
    "        num_imputer.fit_transform(X[numeric_cols]),\n",
    "        columns=numeric_cols,\n",
    "        index=X.index\n",
    "    )\n",
    "    scaler = StandardScaler()\n",
    "    X_num_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X_num),\n",
    "        columns=numeric_cols,\n",
    "        index=X.index\n",
    "    )\n",
    "\n",
    "    # categorical handling\n",
    "    low_card_cols = []\n",
    "    high_card_cols = []\n",
    "    freq_maps = {}\n",
    "\n",
    "    for c in cat_cols:\n",
    "        nunique = X[c].nunique(dropna=True)\n",
    "        if nunique <= 20:\n",
    "            low_card_cols.append(c)\n",
    "        else:\n",
    "            high_card_cols.append(c)\n",
    "            freq = X[c].fillna('::MISSING::').value_counts(normalize=True).to_dict()\n",
    "            freq_maps[c] = freq\n",
    "\n",
    "    # one-hot for low-cardinality (use get_dummies)\n",
    "    if low_card_cols:\n",
    "        X_low = pd.get_dummies(X[low_card_cols].fillna('::MISSING::'), dummy_na=False, drop_first=False)\n",
    "    else:\n",
    "        X_low = pd.DataFrame(index=X.index)\n",
    "\n",
    "    # frequency encoding for high-card\n",
    "    X_high = pd.DataFrame(index=X.index)\n",
    "    for c in high_card_cols:\n",
    "        X_high[c + \"_freq\"] = X[c].fillna('::MISSING::').map(freq_maps[c]).fillna(0.0)\n",
    "\n",
    "    # concat all\n",
    "    X_proc = pd.concat([X_num_scaled, X_low, X_high], axis=1)\n",
    "    # Save metadata\n",
    "    meta = {\n",
    "        'numeric_cols': numeric_cols,\n",
    "        'num_imputer': num_imputer,\n",
    "        'scaler': scaler,\n",
    "        'low_card_cols': low_card_cols,\n",
    "        'high_card_cols': high_card_cols,\n",
    "        'freq_maps': freq_maps,\n",
    "        'onehot_columns': X_low.columns.tolist()\n",
    "    }\n",
    "    return X_proc, meta\n",
    "\n",
    "def preprocess_transform(X_test, meta):\n",
    "    X = X_test.copy()\n",
    "    numeric_cols = meta['numeric_cols']\n",
    "    num_imputer = meta['num_imputer']\n",
    "    scaler = meta['scaler']\n",
    "\n",
    "    # Ensure numeric_cols exist in test (if missing, create NaN)\n",
    "    for c in numeric_cols:\n",
    "        if c not in X.columns:\n",
    "            X[c] = np.nan\n",
    "\n",
    "    X_num = pd.DataFrame(\n",
    "        num_imputer.transform(X[numeric_cols]),\n",
    "        columns=numeric_cols,\n",
    "        index=X.index\n",
    "    )\n",
    "    X_num_scaled = pd.DataFrame(\n",
    "        scaler.transform(X_num),\n",
    "        columns=numeric_cols,\n",
    "        index=X.index\n",
    "    )\n",
    "\n",
    "    # low-card one-hot: create same columns as training one-hot columns\n",
    "    low_card_cols = meta['low_card_cols']\n",
    "    if low_card_cols:\n",
    "        X_low = pd.get_dummies(X[low_card_cols].fillna('::MISSING::'), dummy_na=False, drop_first=False)\n",
    "        # ensure same columns as train\n",
    "        for c in meta['onehot_columns']:\n",
    "            if c not in X_low.columns:\n",
    "                X_low[c] = 0\n",
    "        X_low = X_low[meta['onehot_columns']]  # same order\n",
    "    else:\n",
    "        X_low = pd.DataFrame(index=X.index)\n",
    "\n",
    "    # high-card frequency encoding\n",
    "    X_high = pd.DataFrame(index=X.index)\n",
    "    for c in meta['high_card_cols']:\n",
    "        fmap = meta['freq_maps'].get(c, {})\n",
    "        X_high[c + \"_freq\"] = X[c].fillna('::MISSING::').map(fmap).fillna(0.0)\n",
    "\n",
    "    X_proc = pd.concat([X_num_scaled, X_low, X_high], axis=1)\n",
    "    return X_proc\n",
    "\n",
    "# ---------- 7. Fit preprocess on train and transform test ----------\n",
    "X_train_proc, meta = preprocess_fit_transform(X_train_raw)\n",
    "X_test_proc = preprocess_transform(X_test_raw, meta)\n",
    "\n",
    "print(\"Processed feature counts:\", X_train_proc.shape[1])\n",
    "\n",
    "# ---------- 8. Balance the training set (SMOTE on numerical features) ----------\n",
    "# SMOTE works on numeric arrays; our features are numeric after preprocessing\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42)   # <-- bỏ n_jobs\n",
    "X_train_bal, y_train_bal = sm.fit_resample(X_train_proc, y_train)\n",
    "\n",
    "print(\"After SMOTE, train shape:\", X_train_bal.shape)\n",
    "print(\"Label distribution after SMOTE:\", pd.Series(y_train_bal).value_counts())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed44e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest results:\n",
      "Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     60000\n",
      "           1       1.00      1.00      1.00     32209\n",
      "\n",
      "    accuracy                           1.00     92209\n",
      "   macro avg       1.00      1.00      1.00     92209\n",
      "weighted avg       1.00      1.00      1.00     92209\n",
      "\n",
      "ROC-AUC: 1.0\n"
     ]
    }
   ],
   "source": [
    "# ---------- 9. Train RandomForest ----------\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train_bal, y_train_bal)\n",
    "y_pred_rf = rf.predict(X_test_proc)\n",
    "print(\"\\nRandom Forest results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "try:\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_test, rf.predict_proba(X_test_proc)[:,1]))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd55d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\firek\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [23:20:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost results:\n",
      "Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     60000\n",
      "           1       1.00      1.00      1.00     32209\n",
      "\n",
      "    accuracy                           1.00     92209\n",
      "   macro avg       1.00      1.00      1.00     92209\n",
      "weighted avg       1.00      1.00      1.00     92209\n",
      "\n",
      "ROC-AUC: 1.0\n"
     ]
    }
   ],
   "source": [
    "# ---------- 10. Train XGBoost ----------\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb.fit(X_train_bal, y_train_bal)\n",
    "y_pred_xgb = xgb.predict(X_test_proc)\n",
    "print(\"\\nXGBoost results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "try:\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_test, xgb.predict_proba(X_test_proc)[:,1]))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09610c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF conf matrix:\n",
      " [[60000     0]\n",
      " [    0 32209]]\n",
      "XGB conf matrix:\n",
      " [[60000     0]\n",
      " [    0 32209]]\n"
     ]
    }
   ],
   "source": [
    "# ---------- 11. Confusion matrices ----------\n",
    "print(\"RF conf matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"XGB conf matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
